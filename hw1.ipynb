{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXoQir0P3P_L"
   },
   "source": [
    "# 10-714: Homework 1\n",
    "\n",
    "In this homework you will build the basics of a reverse mode automatic differentiation engine which we will refer to as __needle__ (__ne__cessary __e__lements of __d__eep __le__arning). Before we dive into the code, let's setup the connection to Drive just as we did for homework 0, and install any necessary packages. To get started, make a copy of this notebook file by selecting \"Save a copy in Drive\" from the \"File\" menu, and then run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9vtSr-B3RMx",
    "outputId": "10894b4a-cced-49c4-917b-73cc51a07d30"
   },
   "outputs": [],
   "source": [
    "# # Code to set up the assignment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# #%cd /content/drive/MyDrive/\n",
    "# # !mkdir -p 10714\n",
    "# # %cd /content/drive/MyDrive/10714\n",
    "# # TODO: UPDATE THIS\n",
    "# # !git clone https://github.com/dlsys10714/hw1.git\n",
    "# %cd /content/drive/MyDrive/10714/hw1\n",
    "\n",
    "# !pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "# !pip3 install numdifftools\n",
    "\n",
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./python/apps')\n",
    "from simple_ml import *\n",
    "\n",
    "grader_key = 'YOUR_MUGRADE_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `needle`\n",
    "\n",
    "The purpose of `needle` is to enable the creation and manipulation of computation graphs. Computation graphs are composed of `Tensor` objects, each of which having an operation and a value. Furthermore, in future homeworks we will extend what we are developing here to work with multiple hardware backends (e.g. cuda). We have provided you with these base classes, and understanding them will be critical to this homework as you will use this as a starting point. Take time to become familiar with the given `needle` source code and pay special attention to the `Op`, `Value`, `Tensor` classes, which can be found in `src/python/needle/autograd.py` and the `Device` class, which can be found in `src/python/needle/device.py`. \n",
    "\n",
    "As you saw looking through the needle, we have given the following example operators: element-wise addition and scalar addition, element-wise multiplication, and scalar multiplication. All operators are tracked through the OP_TABLE (found in the `src/python/needle/ops.py` file), and have four key elements: \n",
    "\n",
    "1. A class in `src/python/needle/ops.py` that includes a pointer to the forward pass, as well as a function the defines the gradient (or backward pass).  \n",
    "\n",
    "2. A class in the desired device backend that defines the operation itself (or forward pass). \n",
    "\n",
    "3. A method in the Tensor class that allows for the operation to be used on/between instances of Tensor (i.e. the `__add__` method allows two Tensor objects to be added using the `+` symbol). \n",
    "\n",
    "4. `OP_TABLE` registration - this must be done for the class created in the `src/python/needle/ops.py` file, as well as the class defined within the device backend. This ensures that when an operator is called on a `Tensor` object, the correct operation is called on the correct device backend. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18KCMTWT3zVY"
   },
   "source": [
    "## Question 1: Implementing forward computation\n",
    "\n",
    "In the file `src/python/needle/numpy_backend.py` you will find the forward passes for `add`, `add_scalar`, `multiply` and `multiply_scalar`. Note that the arguments to each of these functions is `inputs` and `attrs` - `inputs` is the list of input (numpy) arrays, whereas `attrs` is a dictionary of additional arguments to the function. In order to see what `inputs` and `attrs` should contain for each function, take a look at the corresponding `Op` classes in `src/python/needle/ops.py`: `EWiseAddOp`, `AddScalarOp`, `EWiseMulOp`, and `EWiseMulScalarOp`. For example, for the case of addition by a scalar, we see that the following function is returned when calling the class `AddScalarOp`: `Tensor.make_from_op(self, [a], attrs={\"scalar\": scalar})`. \n",
    "\n",
    "To start, implement and test (with the following code cell) the forward passes of following operators in `src/python/needle/numpy_backend.py`, taking care to look at the corresponding classes in `src/python/needle/ops.py` for the number of arrays passed in `inputs`, and names of additional arguments in `attrs`. Note that these should all be straightforward, and nearly all will be one line calls to the equivalent `numpy` function. \n",
    "\n",
    "\n",
    "- `divide`: true division of the inputs, element-wise\n",
    "- `divide_scalar`: true division of the input by a scalar, element-wise\n",
    "- `matmul`: matrix multiplication of the inputs\n",
    "- `summation`: sum of array elements over given axes\n",
    "- `broadcast_to`: broadcast an array to a new shape\n",
    "- `reshape`: gives a new shape to an array without changing its data\n",
    "- `negate`: numerical negative, element-wise\n",
    "- `transpose`: reverse or permute the axes of an array; returns the modified array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -k \"forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit grader_key -k \"forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Implementing backward computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have those working, let's implement the backward passes for those operators. Each class of operator in `src/python/needle/ops.py` has a `gradient` function that takes in three arguments: `output_grads`, `node`, and `attrs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement the backward pass operations using only needle operations on these needle objects. Specifically, fill in the `gradient` function of the following classes:\n",
    "\n",
    "- `EWiseDivOp`\n",
    "- `DivScalarOp`\n",
    "- `MatMulOp`\n",
    "- `SummationOp`\n",
    "- `BroadcastToOp`\n",
    "- `ReshapeOp`\n",
    "- `NegateOp`\n",
    "- `TransposeOp`\n",
    "\n",
    "TODO - need more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etSSMzD0fee0",
    "outputId": "2e8d28f1-9dee-4ad0-d9c3-9d9bdb1c30ea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -k \"backward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit grader_key -k \"backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQMdWR1x7Euc"
   },
   "source": [
    "## Question 3: Topological sort\n",
    "\n",
    "Now your system is capable of performing operations on tensors which builds up a computation graph. Now we will write one of the main utilities needed for automatic differentiation - the [topological sort](https://en.wikipedia.org/wiki/Topological_sorting). This will allow us to traverse through (forward or reverse) the compuatation graph, computing gradients along the way. Furthermore, the previously built components will allow for the operations we perform during this reverse topological traversal to further add to our computation graph (as discussed in lecture), and will therefore give us higher-order differentiation \"for free.\" \n",
    "\n",
    "Fill out the `find_topo_sort` method and the `topo_sort_dfs` helper method (in `src/python/needle/autograd.py`) to perform this topological sorting. \n",
    "\n",
    "TODO - need more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQqntvth8GVa",
    "outputId": "c8bcdac7-c8e3-47ca-f450-fac21d29bfab"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -k \"topo_sort\" -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPouXyxCfee1",
    "outputId": "803faa1b-a480-4501-8149-b32ff4d1b90b"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit grader_key -k \"topo_sort\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTDpkG098JiR"
   },
   "source": [
    "## Question 4: Gradient computation\n",
    "\n",
    "Now that the topological sort is finished, leverage it to calculate the gradients in our computation graph by filling out the `compute_gradient_of_variables` method in `src/python/needle/autograd.py`. This method should walk through the computation graph in reverse order, accumulating gradients along the way. \n",
    "\n",
    "TODO - need more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L37C7ez8fkM",
    "outputId": "f2846195-0754-465c-97d7-c36a7ade8cea"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -k \"compute_gradient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwjSL3n6fee2",
    "outputId": "8ff6a756-ac4d-4255-bef0-181c7ebbb594"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit grader_key -k \"compute_gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgMkRkJE8i9D"
   },
   "source": [
    "## Question 5: Softmax loss\n",
    "\n",
    "The following questions will be tested using the MNIST dataset, so first copy and paste your solution to Question 2 of Homework 0 to the `parse_mnist` function_ in the `src/python/simple_ml.py` file.  \n",
    "\n",
    "In this question, you will implement the softmax loss as defined in the `softmax_loss()` function in `src/python/needle/simple_ml.py`, which we defined in Question 3 of Homework 0, except this time, the softmax loss takes as input a `Tensor` of logits and a `Tensor` of one hot encodings of the true labels. \n",
    "\n",
    "You will first need to implement the forward and backward passes of two additional operators: ``log`` and ``exp``. Begin by filling out the functions `log`, and `exp` in `src/python/needle/numpy_backend.py` and then fill out the `gradient` function of the classes `LogOp` and `ExpOp` in `src/python/needle/ops.py`. \n",
    " \n",
    "Once those operators have been implemented, implement the function `softmax_loss` in `src/python/simple_ml.py`. You can start with your solution from Homework 0, and then modify it to be compatible with `needle` objects and operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CpwK-c989OkI",
    "outputId": "0f145d00-6dce-46c9-d5f4-5d72e6d65419"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -k \"softmax_loss_ndl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTwK4X6bfee2",
    "outputId": "6e666bd5-7f75-461b-b42a-673512ba953d"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit grader_key -k \"softmax_loss_ndl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWGd-mr-9luT"
   },
   "source": [
    "## Question 6: SGD for a two-layer neural network\n",
    "\n",
    "As you did in Homework 0, you will now implement stochastic gradient descent (SGD) for a simple two-layer neural network as defined in Question 5 of Homework 0. \n",
    "\n",
    "First, you will need to implement the forward and backward passes of the `relu` operator. Begin by filling out the function `relu` in `src/python/needle/numpy_backend.py` and then fill out the `gradient` function of the class `ReLUOp` in `src/python/needle/ops.py`. \n",
    "\n",
    "Then, fill out the `nn_epoch` method in the `src/python/simple_ml.py` file. Again, you can use your solution in Homework 0 for the `nn_epoch` function as a starting point. Note that unlike in Homework 0, the inputs `W1` and `W2` are Tensors. Unlike the previous homework, your solution should return the `W1` and `W2` `Tensors`. Inputs `X` and `y` however are still numpy arrays - iterate over minibatches of the numpy arrays `X` and `y` as you did in Homework 0, and then cast `X_batch` as a `Tensor`, and one hot encode `y_batch` and cast as a `Tensor`. Later in the course we will build iterators or data loaders for `Tensor` data. Use the `softmax_loss` you wrote in Question 5, and compute the gradients by calling the `.backward` method of the `Tensor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHKC82ex9wFg",
    "outputId": "5a319557-624e-45be-e839-3e1d30efb6f5"
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -k \"nn_epoch_ndl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVt8QXrcfee3",
    "outputId": "fad588b5-4f02-4219-84c1-6c9e30be1650"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit YOUR_GRADER_KEY_HERE -k \"nn_epoch_ndl\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1_combined.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
